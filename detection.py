from accessoryFunctions.accessoryFunctions import printtime
import jellyfish
import shutil
import os
import pysam
import genome_size
import gzip
import bz2
import subprocess
import run_clark
# from Bio.SeqUtils import GC


# TODO: Add option to try to remove reads that have bad kmers in them - maybe not necessary, but could be useful.
class ContamDetect:

    @staticmethod
    def parse_fastq_directory(fastq_folder):
        """
        Should be the first thing called on a ContamDetect object.
        :return: List of fastqpairs in nested array [[forward1, reverse1], [forward2, reverse2]] in fastq_pairs,
        list of single-ended files in fastq_singles
        """
        import glob
        # Get a list of all fastq files. For some reason, having/not having the slash doesn't seem to matter on the
        # fastqfolder argument. These should be all the common extensions
        fastq_files = glob.glob(fastq_folder + "/*.fastq*")
        fastq_files += glob.glob(fastq_folder + "/*.fq*")
        fastq_pairs = list()
        fastq_singles = list()
        for name in fastq_files:
            # If forward and reverse reads are present, put them in a list of paired files.
            # May need to add support for other naming conventions too. Supports both _R1 and _1 type conventions.
            if "_R1" in name and os.path.isfile(name.replace("_R1", "_R2")):
                fastq_pairs.append([name, name.replace("_R1", "_R2")])
            # Other naming convention support.
            elif "_1" in name and os.path.isfile(name.replace("_1", "_2")):
                fastq_pairs.append([name, name.replace("_1", "_2")])
            # Assume that if we can't find a mate reads are single ended, and add them to the appropriate list.
            elif '_R2' not in name and '_2' not in name:
                fastq_singles.append(name)

        return fastq_pairs, fastq_singles

    def trim_fastqs(self, fastq_pairs, fastq_singles):
        """
        For each pair of fastqs in list passed, uses bbduk to trim those file, and puts them in a tmp directory.
        :param fastq_files: Fastq_pairs list generated by parse_fastq_directory.
        :return:
        """
        # Figure out where bbduk is so that we can use the adapter file.
        cmd = 'which bbduk.sh'
        bbduk_dir = subprocess.check_output(cmd.split())
        bbduk_dir = bbduk_dir.split('/')[:-1]
        bbduk_dir = '/'.join(bbduk_dir)
        # Iterate through pairs, running bbduk and writing the trimmed output to the tmp folder for this run.
        for pair in fastq_pairs:
            out_forward = self.output_file + 'tmp/' + pair[0].split('/')[-1]
            out_reverse = self.output_file + 'tmp/' + pair[1].split('/')[-1]
            cmd = 'bbduk.sh in1={} in2={} out1={} out2={} qtrim=w trimq=20 k=25 minlength=50 forcetrimleft=15' \
                  ' ref={}/resources/adapters.fa hdist=1 tpe tbo threads={}'.format(pair[0], pair[1], out_forward,
                                                                                    out_reverse, bbduk_dir,
                                                                                    str(self.threads))
            with open(self.output_file + 'tmp/junk.txt', 'w') as outjunk:
                subprocess.call(cmd, shell=True, stderr=outjunk)

        # Go through single reads, and run bbduk on them too.
        for single in fastq_singles:
            out_name = self.output_file + 'tmp/' + single.split('/')[-1]
            cmd = 'bbduk.sh in={} out={} qtrim=w trimq=20 k=25 minlength=50 forcetrimleft=15' \
                  ' ref={}/resources/adapters.fa hdist=1 tpe tbo threads={}'.format(single, out_name,
                                                                                    bbduk_dir, str(self.threads))
            with open(self.output_file + 'tmp/junk.txt', 'w') as outjunk:
                subprocess.call(cmd, shell=True, stderr=outjunk)

    def run_jellyfish(self, fastq, threads):
        """
        Runs jellyfish at kmer length of self.kmer_size. Writes kmer sequences to mer_sequences.fasta
        :param fastq: An array with forward reads at index 0 and reverse reads at index 1. Can also handle single reads,
        just input an array of length 1.
        :return: integer num_mers, which is number of kmers in the reads at that kmer size.
        """
        # Send files to check if they're compressed. If they are, create uncompressed version that jellyfish can handle.
        to_remove = list()
        to_use = list()
        for j in range(len(fastq)):
            uncompressed = ContamDetect.uncompress_file(fastq[j])
            if 'bz2' in fastq[j]:
                to_use.append(fastq[j].replace('bz2', ''))
                to_remove.append(fastq[j].replace('.bz2', ''))
            elif 'gz' in fastq[j]:
                to_use.append(fastq[j].replace('.gz', ''))
                to_remove.append(fastq[j].replace('.gz', ''))
            else:
                to_use.append(fastq[j])
        # Run jellyfish! Slightly different commands for single vs paired-end reads.
        if len(to_use) > 1:
            cmd = 'jellyfish count -m ' + str(self.kmer_size) + ' -s 100M --bf-size 100M -t ' + str(threads) + ' -C -F 2 ' +\
                  to_use[0] + ' ' + to_use[1]
        else:
            cmd = 'jellyfish count -m ' + str(self.kmer_size) + ' -s 100M --bf-size 100M -t ' + str(threads) + ' -C -F 1 ' + \
                  to_use[0]
        # os.system(cmd)
        subprocess.call(cmd, shell=True)
        # Get the mer_counts file put into the tmp folder that ends up being deleted.
        os.rename('mer_counts.jf', self.output_file + 'tmp/mer_counts.jf')
        # If we had to uncompress files, remove the uncompressed versions.
        if uncompressed:
            for f in to_remove:
                try:
                    # print(f)
                    os.remove(f)
                except:# Needed in case the file has already been removed - figure out the specific error soon.
                    pass

    # TODO: Consider changing this to just calling jellyfish dump, since then you wouldn't have to get the python
    # jellyfish thing installed (and then you could use python3!).
    def write_mer_file(self, jf_file):
        """
        :param jf_file: .jf file created by jellyfish to be made into a fasta file
        :return: The number of unique kmers in said file.
        """
        mf = jellyfish.ReadMerFile(jf_file)
        # print('Writing mer sequences to file...')
        outstr = list()
        i = 1
        for mer, count in mf:
            if count > 3:
                outstr.append('>mer' + str(i) + '_' + str(count) + '\n')
                outstr.append(str(mer) + '\n')
            i += 1
        f = open(self.output_file + 'tmp/mer_sequences.fasta', 'w')
        f.write(''.join(outstr))
        f.close()
        num_mers = i
        return num_mers

    def run_bbmap(self, pair, threads):
        """
        Runs bbmap on mer_sequences.fasta, against mer_sequences.fasta, outputting to test.sam. Important to set
        ambig=all so kmers don't just match with themselves. The parameter pair is expected to be an array with forward
        reads at index 0 and reverse at index 1. If you want to pass single-end reads, just need to give it an array of
        length 1.
        """
        if os.path.isdir('ref'):
            shutil.rmtree('ref')
        cmd = 'bbmap.sh ref=' + self.output_file + 'tmp/mer_sequences.fasta in=' + self.output_file + 'tmp/mer_sequences.fasta ambig=all ' \
              'outm=' + self.output_file + 'tmp/' + pair[0].split('/')[-1] + '.sam idfilter=0.96 minid=0.95 subfilter=1 insfilter=0 ' \
                                                     'delfilter=0 indelfilter=0 nodisk threads=' + str(threads)
        # os.system(cmd)
        # print('Running bbmap...')
        with open(self.output_file + 'tmp/junk.txt', 'w') as outjunk:
            subprocess.call(cmd, shell=True, stderr=outjunk)

    @staticmethod
    def uncompress_file(filename):
        """
        If a file is gzipped or bzipped, creates an uncompressed copy of that file in the same folder
        :param filename: Path to file you want to uncompress
        :return: True if the file needed to be uncompressed, otherwise false.
        """
        uncompressed = False
        if ".gz" in filename:
            in_gz = gzip.open(filename, 'rb')
            out = open(filename.replace('.gz', ''), 'w')
            out.write(in_gz.read())
            out.close()
            uncompressed = True
        elif ".bz2" in filename:
            in_bz2 = bz2.BZ2File(filename, 'rb')
            out = open(filename.replace('.bz2', ''), 'w')
            out.write(in_bz2.read())
            out.close()
            uncompressed = True
        return uncompressed

    def read_samfile(self, num_mers, fastq):
        """
        :param num_mers: Number of unique kmers for the sample be looked at. Found by write_mer_file.
        :param fastq: Array with forward read filepath at index 0, reverse read filepath at index 1. Alternatively, name
        of single-end read file in array of length 1.
        Parse through the SAM file generated by bbmap to find how often contaminating alleles are present.
        Also calls methods from genome_size.py in order to estimate genome size (good for finding cross-species contam).
        Writes results to user-specified output file.
        """
        i = 1
        # Open up the alignment file for parsing.
        samfile = pysam.AlignmentFile(self.output_file + 'tmp/' + fastq[0].split('/')[-1] + '.sam', 'r')
        # samfile = pysam.AlignmentFile('test.sam', 'r')
        for match in samfile:
            # We're interested in full-length matches with one mismatch. This gets us that.
            if "1X" in match.cigarstring and match.query_alignment_length == self.kmer_size:
                query = match.query_name
                reference = samfile.getrname(match.reference_id)
                query_kcount = float(query.split('_')[-1])
                ref_kcount = float(reference.split('_')[-1])
                if query_kcount > ref_kcount:
                    high = query_kcount
                    low = ref_kcount
                else:
                    low = query_kcount
                    high = ref_kcount
                # Ratios that are very low are likely sequencing errors, and high ratios are likely multiple similar
                # genes within a genome (looking at you, E. coli!)
                if 0.01 < low/high < 0.3:
                        # print(query, reference)
                        i += 1
        # Try to get estimated genome size.
        # Make jellyfish run a histogram.
        genome_size.run_jellyfish_histo(self.output_file)
        # Find total number of mers and peak coverage value so estimated genome size can be calculated.
        peak, total_mers = genome_size.get_peak_kmers(self.output_file + 'tmp/histogram.txt')
        # Calculate the estimated size
        estimated_size = genome_size.get_genome_size(total_mers, peak)
        # Large estimated size means cross species contamination is likely. Run CLARK-light to figure out which species
        # are likely present
        if estimated_size > 10000000 and self.classify:
            printtime('Cross-species contamination suspected! Running CLARK for classification.', self.start)
            run_clark.classify_metagenome('bacteria/', fastq, self.threads)
            clark_results = run_clark.read_clark_output('abundance.csv')
        else:
            clark_results = 'NA'
        # Estimate coverage with some shell magic.
        estimated_coverage = ContamDetect.estimate_coverage(estimated_size, fastq)
        f = open(self.output_file, 'a+')
        # Calculate how often we have potentially contaminating kmers.
        percentage = (100.0 * float(i)/float(num_mers))
        f.write(fastq[0].split('/')[-1] + ',' + str(percentage) + ',' + str(num_mers) + ',' + str(estimated_size) + ',' +
                str(estimated_coverage) + ',' + clark_results + '\n')
        f.close()

    @staticmethod
    def estimate_coverage(estimated_size, pair):
        """
        :param estimated_size: Estimated size of genome, in basepairs. Found using genome_size.get_genome_size
        :param pair: Array with structure [path_to_forward_reads, path_to_reverse_reads].
        :return: Estimated coverage depth of genome, as an integer.
        """
        # Use some shell magic to find how many basepairs in forward fastq file - cat it into paste, which lets cut take
        # only the second column (which has the sequence), and then count the number of characters.
        if ".gz" in pair[0]:
            cmd = 'zcat ' + pair[0] + ' | paste - - - - | cut -f 2 | wc -c'
        elif ".bz2" in pair[0]:
            cmd = 'bzcat ' + pair[0] + ' | paste - - - - | cut -f 2 | wc -c'
        else:
            cmd = 'cat ' + pair[0] + ' | paste - - - - | cut -f 2 | wc -c'
        number_bp = int(subprocess.check_output(cmd, shell=True))
        # Multiply by length of array (2 if paired files, 1 if single ended).
        number_bp *= len(pair)
        return number_bp/estimated_size

    def __init__(self, args, start):
        self.fastq_folder = args.fastq_folder
        self.output_file = args.output_file
        self.threads = args.threads
        self.kmer_size = args.kmer_size
        self.classify = args.classify
        self.start = start
        if not os.path.isdir(self.output_file + 'tmp'):
            os.makedirs(self.output_file + 'tmp')
        f = open(self.output_file, 'w')
        f.write('File,Percentage,NumUniqueKmers,EstimatedGenomeSize,EstimatedCoverage,CrossContamination\n')
        f.close()


"""
if __name__ == '__main__':
    import argparse
    import time
    import multiprocessing
    from accessoryFunctions.accessoryFunctions import printtime

    # Check the number of CPUs available on the system to be used by bbmap.
    cpu_count = multiprocessing.cpu_count()
    start = time.time()
    parser = argparse.ArgumentParser()
    parser.add_argument("fastq_folder", help="Folder that contains fastq files you want to check for contamination. "
                                             "Will find any fastq file that contains .fq or .fastq in the filename.")
    parser.add_argument("output_file", help="Base name of the output csv you want to create. (.csv extension is added"
                                            "by the program).")
    parser.add_argument("-k", "--kmer_size", type=int, default=31, help="Size of kmer to use. Experimental feature. "
                                                                        "Probably don't mess with it.")
    parser.add_argument("-t", "--threads", type=int, default=cpu_count, help="Number of CPUs to run analysis on."
                                                                             " Defaults to number of CPUs on the system.")
    parser.add_argument("-c", "--classify", default=False, action='store_true', help='If cross-contamination is suspected, '
                                                                                     'try to classify using CLARK-l. '
                                                                                     'Off by default.')
    parser.add_argument('-tr', '--trim_reads', default=False, action='store_true', help='If enabled, trims reads to'
                                                                                        'remove low quality bases '
                                                                                        'before kmer-izing. Off by '
                                                                                        'default, but highly recommended.')
    arguments = parser.parse_args()
    # Get our contamination detector object going.
    detector = ContamDetect(arguments, start)
    # Get lists of single and paired files.
    paired_files, single_files = ContamDetect.parse_fastq_directory(arguments.fastq_folder)
    # If we're trimming reads, do that for each file and put the trimmed reads into a tmp folder.
    # Then, parse the tmp folder to get new lists of filenames to do work on.
    if arguments.trim_reads:
        printtime('Trimming input fastq files...', start)
        for pair in paired_files:
            for i in range(len(pair)):
                pair[i] = os.path.abspath(pair[i])
        for single in single_files:
            single = os.path.abspath(single)
        ContamDetect.trim_fastqs(detector, paired_files, single_files)
        paired_files, single_files = ContamDetect.parse_fastq_directory(arguments.output_file + 'tmp/')
    # Get a counter started so that we can tell the user how far along we are.
    sample_num = 1
    # Do contamination detection on paired files first.
    for pair in paired_files:
        # Make sure paths are absolute, otherwise bad stuff tends to happen.
        for i in range(len(pair)):
            pair[i] = os.path.abspath(pair[i])
        printtime('Working on sample ' + str(sample_num) + ' of ' + str(len(paired_files) + len(single_files)), start)
        # Run jellyfish to split into mers.
        printtime('Running jellyfish...', start)
        ContamDetect.run_jellyfish(detector, pair, arguments.threads)
        # Write the mers to a file that can be used by bbmap.
        printtime('Writing mers to file...', start)
        num_mers = ContamDetect.write_mer_file(detector, arguments.output_file + 'tmp/mer_counts.jf')
        # Run bbmap on the output mer file
        printtime('Finding mismatching mers...', start)
        ContamDetect.run_bbmap(detector, pair, arguments.threads)
        # Read through bbmap's samfile output to generate our statistics.
        printtime('Generating contamination statistics...', start)
        ContamDetect.read_samfile(detector, num_mers, pair)
        sample_num += 1
    # Essentially the exact same as our paired file parsing.
    for single in single_files:
        # Make sure paths are absolute, otherwise bad stuff tends to happen.
        single = os.path.abspath(single)
        printtime('Working on sample ' + str(sample_num) + ' of ' + str(len(paired_files) + len(single_files)), start)
        # Split reads into mers.
        printtime('Running jellyfish...', start)
        ContamDetect.run_jellyfish(detector, [single], arguments.threads)
        # Write mers to fasta file
        printtime('Writing mers to file...', start)
        num_mers = ContamDetect.write_mer_file(detector, arguments.output_file + 'tmp/mer_counts.jf')
        # Run bbmap on fasta file.
        printtime('Finding mismatching mers...', start)
        ContamDetect.run_bbmap(detector, [single], arguments.threads)
        # Read samfile to generate statistics.
        printtime('Generating contamination statistics...', start)
        ContamDetect.read_samfile(detector, num_mers, [single])
        sample_num += 1

    end = time.time()
    shutil.rmtree(arguments.output_file + 'tmp')
    printtime("Finished contamination detection!", start)
"""
